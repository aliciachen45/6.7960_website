<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>6.7960 Final Project</title>
    <meta property="og:title" content="The Platonic Representation Hypothesis" />
          <meta charset="UTF-8">    <link rel="shortcut icon" href="images/icon.ico">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <!-- page title -->
    <div class="content-margin-container">
        <div class="margin-left-block">
        </div>

        <div class="main-content-block">
            <table class="header" align=left>
                    <tr>
                        <td colspan=4>
                            <span style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">Counting in Vision Models: CNNs, Visual Transformers, and CNN-Transformers </span>
                        </td>
                    </tr>
                    <tr>
                            <td align=left>
                                    <span style="font-size:17px"><a href="your_website">Alicia Chen</a></span>
                            </td>
                            <td align=left>
                                    <span style="font-size:17px"><a href="your_partner's_website">Tara Sarma</a></span>
                            </td>
                    <tr>
                        <td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
                    </tr>
            </table>
        </div>

        <div class="margin-right-block">
        </div>
    </div>

    <div class="content-margin-container" id="intro">
        <div class="margin-left-block">
            <!-- table of contents here -->
            <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
                <b style="font-size:16px">Outline</b><br><br>
                <a href="#intro">Introduction</a><br><br>
                <a href="#lit_review">Convolutional Transformers</a><br><br>
                <a href="#datasets">Dataset</a><br><br>
                <a href="#models">Models</a><br><br>
                <a href="#results">Results</a><br><br>
                <a href="#discussion">Discussion</a><br><br>
                <a href="#implications_and_limitations">Implications and limitations</a><br><br>
            </div>
        </div>
    </div>

    <div class="content-margin-container" id="intro">
        <div class="margin-left-block">
        </div>

        <div class="main-content-block">
            <h1>Introduction</h1>
            Computer vision models have made massive strides in the past decade. Yet, despite the impressive vision models that now exist, many models, largely CNNs which dominate the computer vision landscape, excel in extracting local features but struggle with modeling long-range dependencies, exhibiting a so-called “texture bias” with a disproportionate focus on smaller details <a href = "#ref_1">[1] </a>. <br><br>
            
            An alternative to CNNs is visual transformers, which are reported to have an overall shape bias, 
            i.e. focus on larger aspects of the image <a href = "#ref_1">[1] </a>. Visual transformers can identify the core features for classification and model global dependencies effectively through self-attention mechanisms but struggle to catch the smaller features. Overlooking the smaller details can consequently lead to an incomplete or even misleading interpretation of the data. In addition to its inability to find smaller details, visual transformers have been reported to be an overall more computationally costly model to train than CNNs, whose more domain-specific architecture makes training much more efficient <a href = "#ref_2">[2] </a>. <br><br>
            
            Given these challenges, we work towards a solution such that a vision model can both “understand” the main features of an image and still “perceive” the smaller details. Specifically, we aim to assess how well current models perform on this by testing the counting ability of these models. Counting tasks in deep learning focus on estimating the number of objects or features in an image or scene, with applications ranging from crowd counting and vehicle detection to cell counting in medical imaging. This problem poses unique challenges due to variations in object scale, occlusion, and cluttered backgrounds.
            Some current vision models are still worse at counting in images than most human <a href = "#ref_3">[3] </a>; generative models, for example, will often give hands the wrong number of fingers. Counting features in an image is one way to integrate all parts of an image into a whole quantitatively, testing a model’s capacity to recognize and incorporate both the smaller and larger details. We aim to test counting through a variety of training tasks on CNNs, visual transformers, and Convolutional Transformers, and we hope the final architecture will be a “middle” ground of the two architectures by efficiently capturing the strengths in both and thus be better suited to the counting task overall. 
            <br><br>
            
        </div>

        <div class="margin-right-block">
        </div>
    </div>

    <div class="content-margin-container" id="lit_review">
        <div class="margin-left-block">
        </div>

        <div class="main-content-block">
            <h1>Convolutional Transformers</h1>
            Recent advancements have integrated convolutional layers with transformers to combine CNNs' local feature extraction with the global context modeling of transformers. For example, the CvT (Convolutional Vision Transformer) by Wu et al. <a href = "#ref_4">[4] </a> replaces standard linear projections in the transformer with convolutions, enabling better local inductive bias and reduced computational overhead. By incorporating convolutions in the early layers of the network, CvT can efficiently extract local features at different scales, while the self-attention mechanism in the transformer layers helps to capture global context and relationships between objects in the image. This combination makes CvT particularly effective for counting tasks in crowded or cluttered scenes, where both fine-grained local information and global context are crucial for accurate object estimation. <br><br>
            
            <div class="image" id = 'CvT_arc'>
                <img src="images/CVTArchitecture.svg" alt="CvT Architecture">
                <div class="caption">Fig 1. Demonstrates the general structure of the CvT used in our methods. Figure is taken from https://github.com/microsoft/CvT</div>
            </div>
            Despite the successes of CvT in counting tasks, several challenges remain. One major limitation is the need for large annotated datasets to train models effectively, particularly in specialized counting tasks like medical imaging or wildlife monitoring. Using simpler datasets is essential to evaluate models like CvT, as they allow for isolating specific weaknesses without the added complexity of real-world noise. By testing on easier datasets with clear, well-separated objects, it's easier to identify where a model fails, whether due to issues with feature extraction, attention mechanisms, or overfitting. This controlled environment helps pinpoint performance bottlenecks, offering valuable insights for further refinement. 

        </div>

        <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
        </div>
    </div>

    <div class="content-margin-container" id="datasets">
        <div class="margin-left-block">
        </div>

        <div class="main-content-block">
            <h1>Dataset</h1>
            We decided to generate our own dataset attuned to our specific counting task in assessing a model’s ability to identify large and small-scale features and to minimize the additional noise present in many natural images. We designed three distinct versions, each tailored to investigate a specific question. Each dataset comprises 256 x 256 images, featuring a pattern of shapes set against a black background. <br><br>

            In both Type 1 and Type 2, we include 5 possible shape classes: circle, triangle, square, convex pentagon, and flag (which 
            we define to be a concave pentagon with exactly one vertex pointing inward). Each chosen individual shape can be of a small, 
            medium, or large size, and is randomly augmented in terms of rotation and flipping. In these two dataset types, our images 
            are grayscale and the shapes themselves are white. 

            <h4 class = "heading4">Type 1</h4>
            The simplest type of image contains up to 3 different shape classes and n individual shapes (n ~ Unif(4, 8)). The shapes are 
            non-overlapping and each of the 2500 training and 500 testing images are generated by sampling possible locations on the 
            256 x 256 grid of pixels, and rejecting the location if it does not fit without overlap. We cap the maximum number of trials 
            per individual shape at 20. This method introduces a positional invariance as the location of the shapes is dynamic. In this task, 
            the target label is n, and we wonder whether a model can accurately count the number of shapes (regardless of shape and size).

            <h4 class = "heading4">Type 2</h4>
            This image type is similar to Type 1, though we wonder whether a model can count the number of shapes from a specific shape class. 
            The generation process is the same as of Type 1, except that we also include the target shape in the upper left corner with an 
            inverted color scheme (the shape is black and the background color in the local vicinity is white). In each image, the target shape 
            is of medium size. 

            <h4 class = "heading4">Type 3</h4>
            The third image type differs significantly from the first two, and we create 1000 training images and 500 testing images for this 
            dataset. This task focuses on whether a model can differentiate between sizes of the same shape and understand the concept of attachment. 
            Analogous to a vision model counting the number of fingers on a hand, we aim to test whether a model can count the number of smaller 
            shapes directly attached to a larger shape while ignoring irrelevant shapes.  <br><br>

            Each image contains only one type of shape (either circles or squares) and exhibits up to three levels of hierarchy:
            <ul>
                <li><b>For circles:</b> 
                    The central circle may have up to 4 smaller circles directly attached, and each of these smaller circles may, in turn, have up 
                    to 4 even smaller circles attached.
                </li><b>For squares: </b>
                    The central square may have one square attached to each of its edges. These attached squares may then have additional smaller 
                    squares attached to their open edges (up to 3 edges per square).
            </ul>
            The target shape is highlighted in red, and the task is to count the number of smaller shapes immediately attached to this target shape 
            (i.e., in the next hierarchical layer). For example, in Figure xc, the target shape has a label of 1 (indicating one attached shape). 
            In Figure xd, the target shape has a label of 3 (indicating three attached shapes). The target shape is restricted to the first or second 
            layer in the hierarchy.  <br><br>




        </div>

        <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="datasets_images">
        <div class="margin-left-block">
        </div>

        <div class="main-content-block">
            <div class="gallery">
                <div class="gallery-item" id = 'image_1'>
                    <img src="images/dataset1_example.png" alt="Image 1">
                    <div class="caption">Fig. 2a</div>
                </div>
                <div class="gallery-item">
                    <img src="images/dataset2_example.png" alt="Image 2">
                    <div class="caption">Fig. 2b</div>
                </div>
                <div class="gallery-item">
                    <img src="images/dataset3_circle_example.png" alt="Image 3">
                    <div class="caption">Fig. 2c</div>
                </div>
                <div class="gallery-item">
                    <img src="images/dataset3_square_example.png" alt="Image 4">
                    <div class="caption">Fig. 2d</div>
                </div>
            </div>
        </div>

        <div class="margin-right-block" style = "font-size: 9pt">
            Fig 2a. Example of dataset 1. This image has three shape types - convex pentagon, flag, and pentagon - 
            and a target label of 7. <br><br>
            
            Fig 2b. Example of dataset 2. This image has a target shape of triangle, a target label of 3, and 
            contains other miscellaneous shape types of a circle and square. <br><br>

            Fig 2c. Example of dataset 3 with circles. <br><br>
    
            Fig 2d. Example of dataset 3 with squares. <br><br>

        </div>
    </div>
    
    
    <div class="content-margin-container" id="models">
        <div class="margin-left-block">
        </div>

        <div class="main-content-block">
            <h1>Models</h1>
            We implemented three model architectures and performed the same evaluation and experiments on all three: 1) convolutional neural network (CNN) 2) visual transformer (ViT) and 3) Convolutional Visual Transformer (CvT). A single ReLU output was added to each to produce a single number (count) which was rounded when evaluating accuracy. <br><br>

            For each experiment, we used the Adam optimizer with a cosine-annealing learning-rate scheduler to prevent the model from becoming stuck in local minima. We also used a constant batch size of 64 and a RMSE (Root Mean Squared) loss function. We found this worked better than MSELoss, especially for CNNs. This is possibly due to RMSE being more linear. 
        </div>

        <div class="margin-right-block" style = "font-size: 9pt">
            The CNN had 6 convolutional layers, a fully-connected linear layer with a latent dimension of length 16. The ViT had 4 attention layers, with an embedding dimension size of 64 and 16x16 patch embeddings. The CvT had 3 attention layers, with increasing embedding dimensions of 16, 48, and 96. It also had decreasing convolutional embedding kernel sizes of 31, 15, and 3, along with strides of 16, then 8, then 1.
        </div>
    </div>


    <div class="content-margin-container" id="results">
        <div class="margin-left-block">
        </div>

        <div class="main-content-block">
            <h1>Results</h1>
            <h4 class = "heading4">Model Loss and Accuracies</h4>
            The training and validation accuracies for each of the three models are shown in <a href = "#table_1">Table 1</a> as well as the bar chart in <a href = "#fig_3">Fig. 3</a>
            <br><br>

            <!-- model loss + accuracy table -->
            <table class="styled-table">
                <thead>
                    <tr>
                        <th></th>
                        <th>Task 1 Training </th>
                        <th>Task 1 Testing </th>
                        <th>Task 2 Training  </th>
                        <th>Task 2 Testing  </th>
                        <th>Task 3 Training  </th>
                        <th>Task 3 Testing  </th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>CNN</td>
                        <td><b>0.981</b></td>
                        <td><b>0.724</b></td>
                        <td><b>0.915</b></td>
                        <td><b>0.296</b></td>
                        <td><b>0.969</b></td>
                        <td>0.668</td>
                    </tr>
                    <tr>
                        <td>ViT</td>
                        <td>0.641</td>
                        <td>0.560</td>
                        <td>0.286</td>
                        <td>0.294</td>
                        <td>0.457</td>
                        <td>0.602</td>
                    </tr>
                    <tr>
                        <td>CvT</td>
                        <td>0.783</td>
                        <td>0.518</td>
                        <td>0.657</td>
                        <td>0.276</td>
                        <td>0.751</td>
                        <td><b>0.670</b></td>
                    </tr>
                </tbody>
            </table>
            <div class = 'caption' style = "padding-left:100px" id = 'table_1'>Table 1. Model Loss and Accuracies</div>
            
            <!-- bar plot of accuracies  -->
            <div class = "float-container" id = "fig_3">
                <div class="legend">
                    <div class="legend-item">
                        <div class="legend-color" style="background: #4caf50;"></div>
                        <div class="legend-label">CNN</div>
                    </div>
                    <div class="legend-item">
                        <div class="legend-color" style="background: #2196f3;"></div>
                        <div class="legend-label">ViT</div>
                    </div>
                    <div class="legend-item">
                        <div class="legend-color" style="background: #ff5722;"></div>
                        <div class="legend-label">CvT</div>
                    </div>
                </div>
                <div class = "float-child">
                    <h3>Training Accuracies</h3>
                    <div class="bar-chart">
                        <!-- Group 1 -->
                        <div class="group">
                            <div class="bar" style="--height: 98.1; --color: #4caf50;">
                                <div class="bar-value">0.981</div>
                                <span></span>
                                <div class="bar-label">Task 1 </div>
                            </div>
                            <div class="bar" style="--height: 64.1; --color: #2196f3;">
                                <div class="bar-value">0.641</div>
                                <span></span>
                                <div class="bar-label">Task 1</div>
                            </div>
                            <div class="bar" style="--height: 78.3; --color: #ff5722;">
                                <div class="bar-value">0.783</div>
                                <span></span>
                                <div class="bar-label">Task 1</div>
                            </div>
                            
                        </div>
                        <!-- Group 2 -->
                        <div class="group">
                            <div class="bar" style="--height: 91.4; --color: #4caf50;">
                                <div class="bar-value">0.914</div>
                                <span></span>
                                <div class="bar-label">Task 2</div>
                            </div>
                            <div class="bar" style="--height: 28.6; --color: #2196f3;">
                                <div class="bar-value">0.286</div>
                                <span></span>
                                <div class="bar-label">Task 2</div>
                            </div>
                            <div class="bar" style="--height: 65.7; --color: #ff5722;">
                                <div class="bar-value">0.657</div>
                                <span></span>
                                <div class="bar-label">Task 2</div>
                            </div>
                        </div>
                        <!-- Group 3 -->
                        <div class="group">
                            <div class="bar" style="--height: 96.9; --color: #4caf50;">
                                <div class="bar-value">0.969</div>
                                <span></span>
                                <div class="bar-label">Task 3</div>
                            </div>
                            <div class="bar" style="--height: 45.7; --color: #2196f3;">
                                <div class="bar-value">0.457</div>
                                <span></span>
                                <div class="bar-label">Task 3</div>
                            </div>
                            
                            <div class="bar" style="--height: 75.1; --color: #ff5722;">
                                <div class="bar-value">0.751</div>
                                <span></span>
                                <div class="bar-label">Task 3</div>
                            </div>
                        </div>
                    </div>
                </div>


                <div class = "float-child">
                    <h3>Testing Accuracies</h3>
                    <div class="bar-chart">
                        <!-- Group 1 -->
                        <div class="group">
                            <div class="bar" style="--height: 72.4; --color: #4caf50;">
                                <div class="bar-value">0.724</div>
                                <span></span>
                                <div class="bar-label">Task 1</div>
                            </div>

                            <div class="bar" style="--height: 56; --color: #2196f3;">
                                <div class="bar-value">0.560</div>
                                <span></span>
                                <div class="bar-label">Task 1</div>
                            </div>

                            <div class="bar" style="--height: 51.8; --color: #ff5722;">
                                <div class="bar-value">0.518</div>
                                <span></span>
                                <div class="bar-label">Task 1</div>
                            </div>
                        </div>
                        <!-- Group 2 -->
                        <div class="group">
                            <div class="bar" style="--height: 29.6; --color: #4caf50;">
                                <div class="bar-value">0.296</div>
                                <span></span>
                                <div class="bar-label">Task 2</div>
                            </div>
                            
                            <div class="bar" style="--height: 29.3; --color: #2196f3;">
                                <div class="bar-value">0.293</div>
                                <span></span>
                                <div class="bar-label">Task 2</div>
                            </div>
                            
                            <div class="bar" style="--height: 27.6; --color: #ff5722;">
                                <div class="bar-value">0.276</div>
                                <span></span>
                                <div class="bar-label">Task 2</div>
                            </div>
                        </div>
                        <!-- Group 3 -->
                        <div class="group">
                            <div class="bar" style="--height: 66.8; --color: #4caf50;">
                                <div class="bar-value">0.668</div>
                                <span></span>
                                <div class="bar-label">Task 3</div>
                            </div>

                            <div class="bar" style="--height: 60.2; --color: #2196f3;">
                                <div class="bar-value">0.602</div>
                                <span></span>
                                <div class="bar-label">Task 3</div>
                            </div>

                            <div class="bar" style="--height: 67; --color: #ff5722;">
                                <div class="bar-value">0.670</div>
                                <span></span>
                                <div class="bar-label">Task 3</div>
                            </div>
                        </div>
                    </div>
                </div>
                <div class = "caption" style = "margin-left:20px">Fig 3. Demonstrates the training and testing accuracies of Tasks 1, 2, 3</div>
            </div>

            <h4 class = "heading4">t-SNE plots (t-distributed Stochastic Neighbor Embedding)</h4>
            <div class = "vertical-gallery">
                <div class = "vertical-gallery-item">
                    <img src="images/cnn_tsne.jpeg" alt="t-SNE CNN" width = 1000px>
                    <div class="caption" style="text-align:left; margin-left:100pt; margin-bottom: 10pt">Fig 4a. t-SNE plot of CNN</div>
                </div>
                <div class = "vertical-gallery-item">
                    <img src="images/vit_tsne.jpeg" alt="t-SNE CNN" width = 1000px>
                    <div class="caption" style="text-align:left; margin-left:100pt; margin-bottom: 10pt">Fig 4b. t-SNE plot of ViT</div>
                </div>
                <div class = "vertical-gallery-item">
                    <img src="images/cvt_tsne.jpeg" alt="t-SNE CNN" width = 1000px>
                    <div class="caption" style="text-align:left; margin-left:100pt; margin-bottom: 10pt">Fig 4c t-SNE plot of CvT</div>
                </div>
                <div class = "caption" style="text-align: left; font-size: 8pt;"> Caption: TSNE plots of the “latent vectors” of each model on testing data for each task, referring to the penultimate layer of each model before the ReLU output. For the CNN, this was the penultimate linear layer, and for the transformers, this was the embeddings of the final attention blocked passed through an average pooling layer. TSNE maps a series of vectors to a two dimensional plane statistically such that vectors with more similar features are closer together.  All plots are colored by ground truth labels, i.e. the “count” produced by the task. 4a: The TSNE plot for CNNs. Latent vectors were of length 16. 4b: The TSNE plot for ViTs. Latent vectors were of length 64. 4c: The TSNE plot for CvTs. Latent vectors were of length 96.
                </div>
            </div>


            <div class = "vertical-gallery">
                <div class = "vertical-gallery-item">
                    <img src="images/task2_tsne.jpeg" alt="t-SNE CNN" width = 1000px>
                    <div class="caption" style="text-align:left; margin-left:100pt">Fig 5. A comparison of TSNE plots for all three models running the training data of task 2.
                    </div>
                </div>
            </div>
        </div>

        <div class="margin-right-block">
        </div>
    </div>
    </div>


    <div class="content-margin-container" id="discussion">
        <div class="margin-left-block">
        </div>

        <div class="main-content-block">
            <h1>Discussion</h1>
            Motivate your project. What question are you asking. Why is it unanswered so far? What gap in the literature or practice are you filling?
            Why is it important?
        </div>

        <div class="margin-right-block">
        </div>
    </div>


    <div class="content-margin-container" id="implications_and_limitations">
        <div class="margin-left-block">
        </div>

        <div class="main-content-block">
            <h1>Implications and limitations</h1>
            Let's end with some discussion of the implications and limitations.
        </div>

        <div class="margin-right-block">
        </div>
    </div>

    <div class="content-margin-container" id="citations">
        <div class="margin-left-block">
        </div>

        <div class="main-content-block">
            <div class='citation' id="references" style="height:auto"><br>
                <span style="font-size:16px">References:</span><br><br>
                <a id="ref_1"></a>[1] Maurício, J., Domingues, I., & Bernardino, J. (2023). Comparing vision transformers and convolutional neural networks for Image Classification: A Literature Review. Applied Sciences, 13(9), 5521. https://doi.org/10.3390/app13095521<br><br>
                <a id="ref_2"></a>[2] Amjoud, A. B., & Amrouch, M. (2023). Object detection using deep learning, CNNS and Vision Transformers: A Review. IEEE Access, 11, 35479–35516. https://doi.org/10.1109/access.2023.3266093<br><br>
                <a id="ref_3"></a>[3] Rane, S., Ku, A., Baldridge, J., Tenney, I., Griffiths, T. L., & Kim, B. (2024). Can Generative Multimodal Models Count to Ten. 46th Annual Meeting of the Cognitive Science Society.<br><br>
                <a id="ref_4"></a>[4] Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., & Zhang, L. (2021). CVT: Introducing convolutions to Vision Transformers. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 22–31. https://doi.org/10.1109/iccv48922.2021.00009<br><br>
                <a id="ref_5"></a>[5] Amjoud, A. B., & Amrouch, M. (2023). Object detection using deep learning, CNNS and Vision Transformers: A Review. IEEE Access, 11, 35479–35516. https://doi.org/10.1109/access.2023.3266093<br><br>
                <a id="ref_6"></a>[6] Amjoud, A. B., & Amrouch, M. (2023). Object detection using deep learning, CNNS and Vision Transformers: A Review. IEEE Access, 11, 35479–35516. https://doi.org/10.1109/access.2023.3266093<br><br>
            </div>
        </div>

        <div class="margin-right-block">
        <!-- margin notes for reference block here -->
        </div>
    </div>

</body>

</html>